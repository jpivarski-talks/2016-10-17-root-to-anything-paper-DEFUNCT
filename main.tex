\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{array}
\usepackage{xcolor}

\title{Liberating ROOT data through Apache Arrow}
\author{Jim Pivarski}
\date{\today}

\newcolumntype{P}[1]{>{\centering\arraybackslash}b{#1}}

\begin{document}
\maketitle

\section*{Motivation}

The purpose of the ROOT file format is to standardize storage of and access to High Energy Physics (HEP) data in an efficient manner. It has been extraordinarily successful over the past twenty years, forming the bedrock of most HEP experimental frameworks.

The ROOT format owes its success to two main factors: it is a meta-format, allowing specific data formats to be expressed within it, and it serializes complex, nested data structures in a columnar way, so that only the fields that are needed for an analysis will be read from disk.

Meta-formats are not new; HDF and FITS both predate ROOT as self-describing scientific data containers. XML and ROOT were released in the same year (1997), and XML has been widely used in industry, despite its inefficiency for numerical data. More recently, JSON has supplanted XML in many applications and Avro, Thrift, and Protocol Buffers provide leaner (binary) representations of numerical data. These all share XML's record-major layout, like ROOT with split-mode turned off. Columnar file formats such as ORC, Parquet, and Feather are like ROOT with split-mode enabled, aiming to provide sub-second responses to queries on terabytes of data in the Hadoop ecosystem.

These so-called ``Big Data'' formats provide the same features as ROOT with similar or better performance. Unlike ROOT, most have a clear separation between code and specification, with APIs in many programming languages. Avro, for instance, has bindings for 11 languages, Thrift for 19, and Protocol Buffers for 10. ROOT can only be accessed in C++. (The Java-based FreeHEP-RootIO\footnote{\url{http://java.freehep.org/freehep-rootio/}} and independent RIO\footnote{\url{http://openscientist.lal.in2p3.fr/}} projects seem to be abandoned and do not open modern ROOT files.) Furthermore, the Big Data formats were designed with networking in mind; most of them can be interpreted while being streamed over HTTP. It may have taken the industry decades to catch up, but now the problem of how to quickly deliver vast quantities of structured, numerically intensive data has become an active area of research and commercial investment.

HEP could benefit from this activity, but for two concerns: (1) long-lived data should have a stable format, and industry formats are changing rapidly; (2) many of these formats overlap in functionality, so which should a physicist choose? For instance, Avro, Thrift, and Protocol Buffers are all variants of binary JSON with schemas, developed independently by the Hadoop team, Facebook, and Google, respectively. It would be unwise to convert petabytes of HEP data into Protocol Buffers only to see its popularity eclipsed by Avro, for example. Also, the same ROOT executable reads unsplit (record-major) and split (columnar) ROOT data. In industry, this choice is made by swapping formats and using a different reader.

Fortunately, the situation is not quite this chaotic: new formats interoperate with old formats and many converters are available. The Parquet project takes a very open approach by storing its columnar data with minimal interpretation so that they can be viewed through Avro's, Thrift's, or Protocol Buffers' ``logical'' type systems, making the data conceptually interchangeable.

Moreover, a new project called Apache Arrow seeks to unify the runtime layout of data in memory. The purpose is to share data among analytic engines, such as Spark and Pandas, without saving it to a file or even copying it in memory. For disk formats, this means that $N$ formats may be translated among each other with $N$ translation libraries, rather than $N(N - 1)$.

Apache Arrow grew out of the in-memory data representation developed for Apache Drill\footnote{\url{https://www.mapr.com/blog/apache-arrow-and-value-vectors}} (a low-latency server for interactive analysis on large datasets). This data layout is similar to the one used by Apache Spark\footnote{\url{https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html}} and others, since they are all motivated by a need to efficiently stream data through the CPU cache. Arrow is columnar, packed, and ready for vectorized instructions, while still describing complex, nested trees.

Arrow is in its infancy. At the time of writing, it has been an Apache top-level project for eight months and announced its first version\footnote{\url{https://github.com/apache/arrow/releases/tag/apache-arrow-0.1.0}} two weeks ago (0.1.0 for Java, C++, and Python). However, the project has momentum. It shares developers with Pandas, Parquet, and Spark, and it is seen as the path for completing the Parquet-C++ library and therefore adding fast Parquet I/O to Pandas. Calcite, Cassandra, Drill, Hadoop, HBase, Ibis, Impala, Kudu, Phoenix, and Storm are also involved in the project, to be ready when Arrow becomes a de-facto standard.

Adding Arrow interoperability to ROOT, either as a part of the ROOT application or an add-on library, would add fluidity to HEP data. Once Arrow is established, physicists could mix ROOT analysis functions with Pandas for machine learning, Spark for distributed, iterative analysis (e.g.\ alignment and calibration), or one of the many contenders for low-latency, interactive analysis (e.g.\ ntuple exploration). A layer of flexibility would be added between the file format and runtime environment, making it safer to try new file formats. And perhaps most importantly, it would remove the barrier between the HEP software ecosystem and the new ecosystems rapidly growing around numerical Python and Hadoop.

This paper describes technical details of two file formats, Avro and Parquet, as well as the developing Arrow specification. It presents a roadmap for developing ROOT-Arrow interoperability and concludes with a walkthrough of code that converts generic TTrees from ROOT into Avro, to serve as a guide.

\section*{}









\pagebreak

zig-zag format

47,407 $t\bar{t}$ events (4-vectors and generator-level particles) containing 55,560 muons. (``Error while instantiating TBranchElement'')

blockSize = 32 MB

pageSize = 1 MB

the one field was Muon.pt for all muons

single processor (had to be explicitly forced for Parquet, which uses all available CPU cores by default)

\renewcommand{\arraystretch}{1.2}

\noindent\begin{tabular}{p{2.4 cm} | P{1.2 cm} P{1.9 cm} P{1.2 cm} P{1.8 cm} P{1.9 cm} P{1.2 cm} P{1.8 cm}}
File type & file size (MB) & convert from ROOT (sec) & read all in C++ (sec) & read a field in C++ (sec) & convert from Avro (sec) & read all in Java (sec) & read a field in Java (sec)      \\\hline
ROOT none     & 399 &          110 &           47 &         11.6 &       \textcolor{gray}{didn't} &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} \\
ROOT gzip 1   & 204 &          127 &           48 &         12.0 &          \textcolor{gray}{try} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} \\
ROOT gzip 2   & 208 &          132 &           47 &         11.4 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} \\
ROOT gzip 9   & 202 &          265 &           48 &         11.2 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} \\\hline
Avro none     & 237 &           76 &          113 &          5.4 &          168 &          8.3 &          3.7 \\
Avro snappy   & 198 &           79 &          116 &          6.2 &          172 &          8.8 &          4.3 \\
Avro deflate  & 180 &          142 &          117 &          7.6 &          197 &         13.5 &          8.9 \\
Avro LZMA     & 169 &          285 &          134 &         25.5 &          576 &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} \\\hline
Parquet none  & 210 &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} &          177 &           54 &          2.3 \\
Parquet snappy  & 200 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} &          184 &           54 &          2.4 \\
Parquet gzip  & 176 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} &          199 &           56 &          2.5 \\
\end{tabular}




\end{document}
