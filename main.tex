\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{array}
\usepackage{xcolor}

\title{Liberating ROOT data through Apache Arrow}
\author{Jim Pivarski}
\date{\today}

\newcolumntype{P}[1]{>{\centering\arraybackslash}b{#1}}

\begin{document}
\maketitle

\section*{Motivation}

The purpose of the ROOT file format is to standardize storage of and access to High Energy Physics (HEP) data in an efficient manner. It has been extraordinarily successful over the past twenty years, forming the bedrock of most HEP experimental frameworks.

The ROOT format owes its success to two main factors: it is a meta-format, allowing specific data formats to be expressed within it, and it serializes complex, nested data structures in a columnar way, so that only the fields that are needed for an analysis will be read from disk.

Meta-formats are not new; HDF and FITS both predate ROOT as self-describing scientific data containers. XML and ROOT were released in the same year (1997), and XML has been widely used in industry, despite its inefficiency for numerical data. More recently, JSON has supplanted XML in many applications and Avro, Thrift, and Protocol Buffers provide leaner (binary) representations of numerical data. These all share XML's record-major layout, like ROOT with split-mode turned off. Columnar file formats such as ORC, Parquet, and Feather are like ROOT with split-mode enabled, aiming to provide sub-second responses to queries on terabytes of data in the Hadoop ecosystem.

These so-called ``Big Data'' formats provide the same features as ROOT with similar or better performance. Unlike ROOT, most have a clear separation between code and specification, with APIs in many programming languages. Avro, for instance, has bindings for 11 languages, Thrift for 19, and Protocol Buffers for 10. ROOT can only be accessed in C++. (The Java-based FreeHEP-RootIO\footnote{\url{http://java.freehep.org/freehep-rootio/}} and independent RIO\footnote{\url{http://openscientist.lal.in2p3.fr/}} projects seem to be abandoned and do not open modern ROOT files.) Furthermore, the Big Data formats were designed with networking in mind; most of them can be interpreted while being streamed over HTTP. It may have taken the industry decades to catch up, but now the problem of how to quickly deliver vast quantities of structured, numerically intensive data has become an active area of research and commercial investment.

HEP could benefit from this activity, but for two concerns: (1) long-lived data should have a stable format, and industry formats are changing rapidly; (2) many of these formats overlap in functionality, so which should a physicist choose? For instance, Avro, Thrift, and Protocol Buffers are all variants of binary JSON with schemas, developed independently by the Hadoop team, Facebook, and Google, respectively. It would be unwise to convert petabytes of HEP data into Protocol Buffers only to see its popularity eclipsed by Avro, for example. Also, the same ROOT executable reads unsplit (record-major) and split (columnar) ROOT data. In industry, this choice is made by swapping formats and using a different reader for each.

Fortunately, the situation is not quite this chaotic: new formats interoperate with old formats and many converters are available. The Parquet project takes a particularly open approach by storing its columnar data with minimal interpretation so that they can be viewed through Avro's, Thrift's, or Protocol Buffers' ``logical'' type systems, making the data conceptually interchangeable.

Moreover, a new project called Apache Arrow seeks to unify the runtime layout of data in memory. The purpose is to share data among analytic engines, such as Spark and Pandas, without saving it to a file or even copying it in memory. For disk formats, this means that $N$ formats may be translated among each other with $N$ translation libraries, rather than $N(N - 1)$.

Apache Arrow grew out of the in-memory data representation developed for Apache Drill\footnote{\url{https://www.mapr.com/blog/apache-arrow-and-value-vectors}} (a low-latency server for interactive analysis on large datasets). The data layout is similar to the one used by Apache Spark\footnote{\url{https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html}} and others, since they are all motivated by a need to efficiently stream data through the CPU cache. Arrow is columnar, packed, and ready for vectorized instructions, while still describing complex, nested trees.

Arrow is in its infancy, though. At the time of writing, it has been an Apache top-level project for eight months and announced its first version\footnote{\url{https://github.com/apache/arrow/releases/tag/apache-arrow-0.1.0}} two weeks ago (0.1.0 for Java, C++, and Python). However, the project has momentum. It shares developers with Pandas, Parquet, and Spark, and it is seen as the path for completing the Parquet-C++ library and therefore adding fast Parquet I/O to Pandas. Calcite, Cassandra, Drill, Hadoop, HBase, Ibis, Impala, Kudu, Phoenix, and Storm are also involved in the project, to be ready when Arrow becomes a de-facto standard.

Adding Arrow interoperability to ROOT, either as a part of the ROOT application or an add-on library, would add fluidity to HEP data. Once Arrow is established, physicists could mix ROOT analysis functions with Pandas for machine learning, Spark for distributed, iterative analysis (e.g.\ alignment and calibration), or one of the many distributed databases contending for low-latency, interactive analysis (e.g.\ ntuple exploration at a vast scale). With this new layer of flexibility between file formats and runtime environments, it would be safer to experiment with new file formats. And perhaps most importantly, it would remove the barrier between the HEP software ecosystem and the new ecosystems that are rapidly growing around numerical Python and Hadoop.

This paper starts with an overview of the Avro (record-major) and Parquet (columnar) file formats, followed by a performance comparison with ROOT to show that they are, in some ways, more performant than ROOT for physics data. Next, we present a roadmap for developing ROOT-Arrow interoperability, concluding with a walkthrough of ROOT-to-Avro code demonstrating some of the necessary steps.

\section*{Avro and Parquet file formats}





\pagebreak

\section*{Performance comparison}

\renewcommand{\arraystretch}{1.2}
\noindent\begin{tabular}{p{2.4 cm} | P{1.2 cm} P{1.9 cm} P{1.2 cm} P{1.8 cm} P{1.9 cm} P{1.2 cm} P{1.8 cm}}
File type & file size (MB) & convert from ROOT (sec) & read all in C++ (sec) & read a field in C++ (sec) & convert from Avro (sec) & read all in Java (sec) & read a field in Java (sec)      \\\hline
ROOT none     & 399 &          110 &           47 &         11.6 &       \textcolor{gray}{didn't} &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} \\
ROOT gzip 1   & 204 &          127 &           48 &         12.0 &          \textcolor{gray}{try} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} \\
ROOT gzip 2   & 208 &          132 &           47 &         11.4 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} \\
ROOT gzip 9   & 202 &          265 &           48 &         11.2 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} \\\hline
Avro none     & 237 &           76 &          113 &          5.4 &          168 &          8.3 &          3.7 \\
Avro snappy   & 198 &           79 &          116 &          6.2 &          172 &          8.8 &          4.3 \\
Avro deflate  & 180 &          142 &          117 &          7.6 &          197 &         13.5 &          8.9 \\
Avro LZMA     & 169 &          285 &          134 &         25.5 &          576 &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} \\\hline
Parquet none  & 210 &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} &        \textcolor{gray}{can't} &          177 &           54 &          2.3 \\
Parquet snappy  & 200 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} &          184 &           54 &          2.4 \\
Parquet gzip  & 176 & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} & \textcolor{gray}{$\downarrow$} &          199 &           56 &          2.5 \\
\end{tabular}

\vspace{0.2 cm}
The above table shows the relative file sizes and read/write performance of a sample of 47,407 $t\bar{t}$ Monte Carlo events. The data are not flat ntuples: they contain TClonesArrays of custom class instances. They represent fully reconstructed 4-vectors with enough auxiliary information to perform complete analyses\footnote{\url{https://github.com/mcremone/BaconAnalyzer}}. ROOT 6.06/08, Avro-C and Avro-Java 1.8.1, and Parquet 1.8.1 were used for read and write tests with an SSD disk and one processor with 4~GB of RAM and 1024 kB of CPU cache.

A typical ROOT file uses gzip compression level 1, corresponding to the second line in the table (204~MB). The ``convert from ROOT'' operation is the time required for a {\tt TTree::CloneTree()} (127 sec).

The Avro analogy represents the TClonesArrays as Avro arrays and the C++ class instances as abstract records. Typically, one would use Avro's built-in Snappy compression to balance file size with CPU effort: in this case ROOT's 204~MB compresses down to a 198~MB Avro file. The ``convert from ROOT'' operation is complicated by the fact that it counts ROOT-reading and Avro-writing, but because pure ROOT-reading takes 48~sec, we can infer that ROOT-to-Avro's 79~sec is 48~sec of reading followed by 31~sec of writing. By the same logic, ROOT-to-ROOT is 48~sec of reading followed by 79~sec ($127 - 48$) of writing.

ROOT is faster than the Avro-C library\footnote{\url{https://avro.apache.org/docs/current/api/c/}} for reading back the whole structure. ROOT takes 48~sec to repopulate the TClonesArrays of class instances and Avro-C takes 116~sec. (This analysis used Avro-C's older ``datum'' interface.) However, Avro-C is faster at picking out one field: muon $p_T$ for all muons can be extracted from ROOT in 12.0~sec and Avro in 6.2~sec. (The dataset has 55,560 muons, 1.17 muon per event.)

This may seem surprising because ROOT's file format was designed to be columnar precisely for this purpose. Avro has to read, decompress, and step through the entire record to pick out the muon $p_T$, while ROOT simply seeks to that branch, decompresses it, and delivers it. There are several factors that complicate this comparison. One is that the disk hardware is an SSD, which has a much higher bandwidth than mechanical disk drives. Another is that snappy is a more lightweight compression algorithm than gzip, with an emphasis on CPU efficiency. The LZMA algorithm compresses the data more tightly at a significant cost in reading (25.5~sec). Finally, the Avro read has a strictly linear memory access pattern that favors prefetching to CPU cache. The relatively poor performance of reading all records in Avro-C (and immediately deleting them) may be related to the simple memory allocation and reference counting procedure it uses to track data ownership.

Perhaps more surprising is the Avro read performance in Java: 8.8~sec to read all fields and 4.3~sec to read muon $p_T$, compared with 116 and 6.2~sec, respectively, in Avro-C. The Java implementation of Avro would have received much more developer scrutiny, as it has a central role in Hadoop data processing. Our array-of-records structure thwarts direct object reuse, but the short-lived objects created and destroyed in this test would benefit from Java's generational garbage collection: they might never live beyond the Eden generation, where memory allocation does not involve an $\mathcal{O}(n)$ search through a fragmented memory space.

The corresponding ROOT file performance in Java with FreeHEP-RootIO would be interesting, but this old project raises an ``Error while instantiating TBranchElement'' exception whenever it encounters anything but flat ntuple data.







\pagebreak

zig-zag format



blockSize = 32 MB

pageSize = 1 MB



single processor (had to be explicitly forced for Parquet, which uses all available CPU cores by default)





\end{document}
